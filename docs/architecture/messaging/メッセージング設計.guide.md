# メッセージング設計 ガイド

> **仕様**: トピック定義・スキーマ・パーティション戦略は [メッセージング設計.md](./メッセージング設計.md) を参照。

## Kafka 採用理由

- メッセージブローカーとして Apache Kafka 3.8 を採用する
- 高スループットかつ順序保証が可能で、イベント駆動アーキテクチャとの親和性が高い
- Protobuf + Schema Registry による厳密なスキーマ管理と互換性チェックを実現する

## Strimzi Kafka Operator 採用理由

Kafka クラスタの管理には **Strimzi Kafka Operator** を採用する。

- Kubernetes ネイティブな Kafka 管理が可能（CRD ベースの宣言的管理）
- ローリングアップデートによる無停止アップグレードをサポート
- Kafka クラスタのスケーリング・監視・セキュリティを Operator が自動管理

## Schema Registry 配置の設計背景

Schema Registry を Kafka クラスタと同じ `messaging` Namespace ではなく `k1s0-system` に配置する理由:

Schema Registry は全 Tier（system / business / service）のサービスから共通的に利用される基盤コンポーネントである。[kubernetes設計.md](../../infrastructure/kubernetes/kubernetes設計.md) の Namespace 設計で `k1s0-system` は「system 層のサーバー・DB・Schema Registry」を対象としており、Kafka ブローカー（messaging NS）とは異なるライフサイクル・アクセスポリシーで管理する。

## CQRS 適用方針

CQRS は **トラフィック量の非対称性が大きいサービス** に限定して適用する。読み取り / 書き込み比率が 10:1 を超える場合、または読み取りモデルの最適化が必要な場合に適用を検討する。単純な CRUD では適用しない。

### アーキテクチャ概要

```
Write Side:                      Read Side:
Command → Server → DB (Write)    Kafka → Projector → DB (Read)
                   ↓                                     ↓
                  Kafka Event                       Query API
```

## イベントソーシング適用方針

イベントソーシングは **監査要件が厳しいドメイン** に限定して適用する。全変更履歴の保持が必要、監査証跡がコンプライアンス要件、状態復元・時点指定クエリが必要な場合に適用を検討する。通常の CRUD で十分な場合は適用しない。

## Saga パターン: コレオグラフィ vs オーケストレーション

分散トランザクションには **コレオグラフィ方式** を基本とする。

### コレオグラフィ方式

各サービスが自律的にイベントを発行・購読し、ビジネスフローを実現する。

```
Order Service          Inventory Service        Payment Service
     │                        │                        │
     │  OrderCreated          │                        │
     │───────────────────────▶│                        │
     │                        │  InventoryReserved     │
     │                        │───────────────────────▶│
     │                        │                        │  PaymentCompleted
     │◀───────────────────────────────────────────────│
     │  OrderConfirmed        │                        │
     │                        │                        │
```

### 補償トランザクション

処理失敗時は補償イベントを発行してロールバックする。

```
Order Service          Inventory Service        Payment Service
     │                        │                        │
     │  OrderCreated          │                        │
     │───────────────────────▶│                        │
     │                        │  InventoryReserved     │
     │                        │───────────────────────▶│
     │                        │                        │  PaymentFailed
     │                        │◀───────────────────────│
     │                        │  InventoryReleased     │
     │◀──────────────────────│                        │
     │  OrderCancelled        │                        │
     │                        │                        │
```

### 方式の選定基準

| 方式               | 採用基準                  | メリット                | デメリット              |
| ------------------ | ------------------------- | ----------------------- | ----------------------- |
| コレオグラフィ     | ステップ数 < 5            | 疎結合、スケーラブル    | フロー全体の把握が困難  |
| オーケストレーション | ステップ数 >= 5          | フロー全体の可視性      | Orchestrator が SPOF    |

5 ステップ以上の複雑なワークフローでは、オーケストレーション方式（Saga Orchestrator）を検討する。

### Orchestrator 実装方針

Orchestrator は **Rust で自前の軽量 Orchestrator** を構築する（Temporal 等の外部ワークフローエンジンは導入しない）。外部ワークフローエンジンを導入しない理由は、k1s0 のワークフローが比較的シンプルであり、Temporal 等の導入による運用コストが利点を上回ると判断したためである。

## DLQ 再処理の設計背景

DLQ に滞留したメッセージの再処理は、管理 API エンドポイント経由で行う。自動再投入ではなく手動確認フローとした理由は、障害の根本原因を確認せずに再処理すると同じ失敗が繰り返されるリスクがあるためである。

再処理フロー:

```
1. DLQ メッセージ発生 → Grafana アラート通知
2. GET /admin/dlq/{topic}/messages でメッセージ内容を確認
3. 問題解消後、POST /admin/dlq/{topic}/replay で再処理を実行
4. DLQ トピックからメッセージを読み取り → 元のトピックに再投入
5. 再処理回数が上限（3回）を超えた場合 → *.dlq.archive トピックに移動
```

## Go 実装パターン

### Producer

```go
// internal/infra/messaging/producer.go
package messaging

import (
    "context"

    "github.com/segmentio/kafka-go"
    "google.golang.org/protobuf/proto"
)

type EventProducer struct {
    writer *kafka.Writer
}

func NewEventProducer(brokers []string, topic string) *EventProducer {
    return &EventProducer{
        writer: &kafka.Writer{
            Addr:         kafka.TCP(brokers...),
            Topic:        topic,
            Balancer:     &kafka.Hash{},   // パーティションキーによる分散
            RequiredAcks: kafka.RequireAll, // acks=all
            Async:        false,
        },
    }
}

func (p *EventProducer) Publish(ctx context.Context, key string, event proto.Message) error {
    value, err := proto.Marshal(event)
    if err != nil {
        return fmt.Errorf("marshal event: %w", err)
    }

    return p.writer.WriteMessages(ctx, kafka.Message{
        Key:   []byte(key),
        Value: value,
    })
}
```

### Consumer

```go
// internal/infra/messaging/consumer.go
package messaging

import (
    "context"
    "log/slog"

    "github.com/segmentio/kafka-go"
)

type EventConsumer struct {
    reader *kafka.Reader
    logger *slog.Logger
}

func NewEventConsumer(brokers []string, topic, groupID string, logger *slog.Logger) *EventConsumer {
    return &EventConsumer{
        reader: kafka.NewReader(kafka.ReaderConfig{
            Brokers:  brokers,
            Topic:    topic,
            GroupID:  groupID,
            MinBytes: 1e3,    // 1KB
            MaxBytes: 10e6,   // 10MB
        }),
        logger: logger,
    }
}

func (c *EventConsumer) Consume(ctx context.Context, handler func(context.Context, kafka.Message) error) error {
    for {
        msg, err := c.reader.ReadMessage(ctx)
        if err != nil {
            return fmt.Errorf("read message: %w", err)
        }

        if err := handler(ctx, msg); err != nil {
            c.logger.Error("failed to handle message",
                slog.String("topic", msg.Topic),
                slog.Int("partition", msg.Partition),
                slog.Int64("offset", msg.Offset),
                slog.String("error", err.Error()),
            )
            // DLQ への転送はリトライ後に実施
        }
    }
}
```

## Rust 実装パターン

### Producer

```rust
// src/infra/messaging/producer.rs
use prost::Message;
use rdkafka::producer::{FutureProducer, FutureRecord};
use rdkafka::ClientConfig;
use std::time::Duration;

pub struct EventProducer {
    producer: FutureProducer,
    topic: String,
}

impl EventProducer {
    pub fn new(brokers: &str, topic: &str) -> Self {
        let producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("acks", "all")
            .set("message.timeout.ms", "5000")
            .create()
            .expect("Failed to create producer");

        Self {
            producer,
            topic: topic.to_string(),
        }
    }

    pub async fn publish<M: Message>(&self, key: &str, event: &M) -> Result<(), Box<dyn std::error::Error>> {
        let payload = event.encode_to_vec();

        self.producer
            .send(
                FutureRecord::to(&self.topic)
                    .key(key)
                    .payload(&payload),
                Duration::from_secs(5),
            )
            .await
            .map_err(|(e, _)| e)?;

        Ok(())
    }
}
```

### Consumer

```rust
// src/infra/messaging/consumer.rs
use rdkafka::consumer::{CommitMode, Consumer, StreamConsumer};
use rdkafka::{ClientConfig, Message};
use futures::StreamExt;

pub struct EventConsumer {
    consumer: StreamConsumer,
}

impl EventConsumer {
    pub fn new(brokers: &str, group_id: &str, topics: &[&str]) -> Self {
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("group.id", group_id)
            .set("auto.offset.reset", "earliest")
            .set("enable.auto.commit", "false")
            .create()
            .expect("Failed to create consumer");

        consumer.subscribe(topics).expect("Failed to subscribe");

        Self { consumer }
    }

    pub async fn consume<F, Fut>(&self, handler: F) -> Result<(), Box<dyn std::error::Error>>
    where
        F: Fn(Vec<u8>) -> Fut,
        Fut: std::future::Future<Output = Result<(), Box<dyn std::error::Error>>>,
    {
        let mut stream = self.consumer.stream();

        while let Some(result) = stream.next().await {
            let msg = result?;
            if let Some(payload) = msg.payload() {
                handler(payload.to_vec()).await?;
                self.consumer.commit_message(&msg, CommitMode::Async)?;
            }
        }

        Ok(())
    }
}
```

## ワークフロー定義例（Saga Orchestrator）

```yaml
# workflows/order-fulfillment.yaml
name: order-fulfillment
steps:
  - name: reserve-inventory
    service: inventory-server
    method: InventoryService.Reserve
    compensate: InventoryService.Release
    timeout: 30s
    retry:
      max_attempts: 3
      backoff: exponential

  - name: process-payment
    service: payment-server
    method: PaymentService.Charge
    compensate: PaymentService.Refund
    timeout: 30s
    retry:
      max_attempts: 3
      backoff: exponential

  - name: confirm-order
    service: order-server
    method: OrderService.Confirm
    compensate: OrderService.Cancel
    timeout: 30s
    retry:
      max_attempts: 3
      backoff: exponential
```
