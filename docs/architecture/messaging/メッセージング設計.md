# メッセージング設計

Kafka を中心としたメッセージングおよびイベント駆動アーキテクチャの仕様を定義する。
Tier アーキテクチャの詳細は [tier-architecture.md](../overview/tier-architecture.md) を参照。

## 基本方針

- メッセージブローカーは **Apache Kafka 3.8** を採用する（高スループットかつ順序保証が可能で、イベント駆動アーキテクチャとの親和性が高い。Protobuf + Schema Registry による厳密なスキーマ管理と互換性チェックを実現する）
- メッセージスキーマは **Protobuf** で定義し、Schema Registry で管理する
- イベント駆動アーキテクチャで CQRS・Saga パターンを適用する
- Kafka クラスタは `messaging` Namespace にデプロイする

---

## D-119: Kafka トピック設計

### トピック命名規則

```
k1s0.{tier}.{domain}.{event-type}.{version}
```

| 要素           | 説明                          | 例                        |
| -------------- | ----------------------------- | ------------------------- |
| `k1s0`         | プロジェクトプレフィックス    | 固定                      |
| `tier`         | Tier 名                       | `system`, `business`, `service` |
| `domain`       | ドメイン名                    | `auth`, `order`, `inventory` |
| `event-type`   | イベント種別                  | `created`, `updated`, `command` |
| `version`      | スキーマバージョン            | `v1`, `v2`                |

#### トピック命名例

| トピック名                              | 用途                     |
| --------------------------------------- | ------------------------ |
| `k1s0.service.order.created.v1`         | 注文作成イベント         |
| `k1s0.service.order.updated.v1`         | 注文更新イベント         |
| `k1s0.service.inventory.reserved.v1`    | 在庫引当イベント         |
| `k1s0.business.accounting.entry.v1`     | 会計仕訳イベント         |
| `k1s0.system.auth.login.v1`            | ログインイベント         |
| `k1s0.system.auth.token_validate.v1`  | トークン検証イベント     |
| `k1s0.system.auth.audit.v1`           | 監査ログイベント（汎用） |
| `k1s0.system.auth.permission_denied.v1` | パーミッション拒否イベント |
| `k1s0.system.config.changed.v1` | 設定変更イベント |

#### System Tier 監査ログフロー

| 項目 | 設定 |
| --- | --- |
| トピック名 | `k1s0.system.auth.audit.v1` |
| Producer | auth-server（Go / Rust） |
| Consumer | audit-aggregator |
| パーティションキー | `user_id` |
| パーティション数 | 6（system tier デフォルト） |
| 保持期間 | 90 日（監査ログカテゴリ） |

```
auth-server
  │
  │ Publish (user_id をパーティションキーに使用)
  ▼
Kafka topic: k1s0.system.auth.audit.v1
  │
  │ Subscribe
  ▼
audit-aggregator → 長期保存ストレージ
```

スキーマは `api/proto/k1s0/event/` 配下の各 Proto ファイルで定義する。主要なメッセージ型:

| メッセージ型 | 用途 | Proto ファイル |
| --- | --- | --- |
| `EventMetadata` | 全イベント共通メタデータ | event_metadata.proto |
| `LoginEvent` | ログイン成功/失敗 | auth_events.proto |
| `TokenValidationEvent` | トークン検証結果 | auth_events.proto |
| `PermissionCheckEvent` | パーミッション確認結果 | auth_events.proto |
| `AuditLogRecordedEvent` | 監査ログ記録イベント | auth_events.proto |
| `ConfigChangedEvent` | 設定変更イベント | config_events.proto |
| `EntryCreatedEvent` | 会計仕訳作成イベント | accounting_events.proto |
| `EntryApprovedEvent` | 会計仕訳承認イベント | accounting_events.proto |
| `OrderCreatedEvent` | 注文作成イベント | order_events.proto |
| `OrderItem` | 注文明細（OrderCreatedEvent/OrderUpdatedEvent のサブメッセージ） | order_events.proto |
| `OrderUpdatedEvent` | 注文更新イベント | order_events.proto |
| `OrderCancelledEvent` | 注文キャンセルイベント | order_events.proto |
| `InventoryReservedEvent` | 在庫引当イベント | inventory_events.proto |
| `InventoryReleasedEvent` | 在庫解放イベント | inventory_events.proto |

### パーティション戦略

| Tier     | デフォルトパーティション数 | レプリケーションファクター（prod） |
| -------- | -------------------------- | ---------------------------------- |
| system   | 6                          | 3                                  |
| business | 3                          | 3                                  |
| service  | 3                          | 3                                  |

> **注記**: レプリケーションファクターは prod 環境（3ブローカー）の値。staging/dev 環境ではブローカー数に応じて自動調整される（詳細は後述の「レプリケーション設定」を参照）。

パーティションキーの選定基準:
- **注文系**: `order_id`（同一注文のイベントを順序保証）
- **ユーザー系**: `user_id`（同一ユーザーの操作を順序保証）
- **汎用**: ドメインのエンティティ ID

> **注記**: メッセージングライブラリの `TopicConfig` のデフォルトパーティション数は 3（business/service tier）である。system tier のトピックを作成する場合は、明示的にパーティション数 6 を指定すること。

### Kafka クラスタ構成（Strimzi Kafka Operator）

Kafka クラスタの管理には **Strimzi Kafka Operator** を採用する。

- Kubernetes ネイティブな Kafka 管理が可能（CRD ベースの宣言的管理）
- ローリングアップデートによる無停止アップグレードをサポート
- Kafka クラスタのスケーリング・監視・セキュリティを Operator が自動管理

| 項目         | 設定                                    |
| ------------ | --------------------------------------- |
| Operator     | Strimzi Kafka Operator                  |
| Namespace    | `messaging`                             |
| Helm Chart   | `strimzi-kafka-operator` (strimzi.io)   |

#### 環境別クラスタ構成

| 環境                 | Kafka ブローカー | ZooKeeper ノード | 備考                                                    |
| -------------------- | ---------------- | ---------------- | ------------------------------------------------------- |
| prod                 | 3                | 3                | KRaft モードへの移行は Strimzi の対応状況を見て検討     |
| staging              | 1                | 1                | 最小構成でコスト最適化                                  |
| dev (docker-compose) | 1                | なし             | KRaft モード（ZooKeeper 不要）で単体起動                |

#### Kafka 認証・暗号化

| 環境                 | 認証方式            | 暗号化     | 備考                                                    |
| -------------------- | ------------------- | ---------- | ------------------------------------------------------- |
| prod                 | SASL/SCRAM-SHA-512  | TLS        | mTLS も検討（Strimzi Operator 設定）                    |
| staging              | SASL/SCRAM-SHA-256  | TLS        | prod と同等のセキュリティ                               |
| dev (docker-compose) | なし（PLAINTEXT）   | なし       | ローカル開発環境では認証不要                             |

メッセージングライブラリの `KafkaConfig` / `MessagingConfig` で `security_protocol` および SASL 関連パラメータを設定する。

### レプリケーション設定

prod 環境のデフォルト値:

```
min.insync.replicas = 2
acks = all
```

> **注記**: 上記は prod 環境（3ブローカー）のデフォルト値である。staging / dev 環境ではブローカー数に応じて `min.insync.replicas = 1` に設定する（下表参照）。

| 環境    | ブローカー数 | レプリケーションファクター | min.insync.replicas |
| ------- | ------------ | -------------------------- | ------------------- |
| dev     | 1            | 1                          | 1                   |
| staging | 1            | 1                          | 1                   |
| prod    | 3            | 3                          | 2                   |

### 消費者グループ命名

```
{service-name}.{purpose}
```

| 消費者グループ名                  | サービス       | 用途                     |
| --------------------------------- | -------------- | ------------------------ |
| `order-server.inventory-check`    | order-server   | 在庫確認処理             |
| `inventory-server.order-reserved` | inventory-server | 注文引当処理           |
| `notification-server.order-event` | notification-server | 注文通知            |
| `analytics-server.all-events`     | analytics-server    | 分析用イベント集約  |

### トピック保持ポリシー

| カテゴリ       | 保持期間  | クリーンアップポリシー | 備考                     |
| -------------- | --------- | ---------------------- | ------------------------ |
| ビジネスイベント | 7 日    | delete                 | 通常のイベント           |
| 監査ログ（Kafka トピック） | 90 日 | delete          | コンプライアンス要件（アプリケーション監査ログの長期保存は可観測性設計.md を参照） |
| コンパクト     | 無期限    | compact                | 最新状態の保持           |
| DLQ            | 30 日     | delete                 | 処理失敗メッセージ       |

### DLQ（Dead Letter Queue）設計

処理に失敗したメッセージは DLQ トピックに転送する。

```
k1s0.{tier}.{domain}.{event-type}.{version}.dlq
```

| 項目           | 設定                                    |
| -------------- | --------------------------------------- |
| DLQ トピック名 | 元トピック名 + `.dlq` サフィックス      |
| リトライ回数   | 3 回（Exponential Backoff）             |
| DLQ 保持期間   | 30 日                                   |
| アラート       | DLQ にメッセージ到達時に Teams 通知     |

```
Consumer → 処理失敗 → リトライ (3回) → DLQ トピック → アラート通知
```

#### DLQ メッセージ再処理方式

DLQ に滞留したメッセージの再処理は、管理 API エンドポイント経由で行う。

| 項目                 | 設計                                                              |
| -------------------- | ----------------------------------------------------------------- |
| メッセージ一覧       | `GET /api/v1/dlq/{topic}` — DLQ 内メッセージの一覧取得（ページネーション対応） |
| メッセージ詳細       | `GET /api/v1/dlq/messages/{id}` — 個別メッセージの内容確認       |
| 単一メッセージ再処理 | `POST /api/v1/dlq/messages/{id}/retry` — 個別メッセージの再処理  |
| メッセージ削除       | `DELETE /api/v1/dlq/messages/{id}` — 処理済みメッセージの削除    |
| 一括再処理           | `POST /api/v1/dlq/{topic}/retry-all` — トピック内全メッセージの再処理 |
| 再処理回数上限       | 3 回（超過分はアーカイブ用トピック `*.dlq.archive` に移動）       |
| 監視                 | Grafana ダッシュボードで DLQ メッセージ数を監視し、アラート設定   |

再処理フロー:

```
1. DLQ メッセージ発生 → Grafana アラート通知
2. GET /api/v1/dlq/{topic} でメッセージ一覧を確認
3. GET /api/v1/dlq/messages/{id} で個別メッセージの内容を確認
4. 問題解消後、POST /api/v1/dlq/messages/{id}/retry で個別再処理、または POST /api/v1/dlq/{topic}/retry-all で一括再処理を実行
5. 再処理回数が上限（3回）を超えた場合 → *.dlq.archive トピックに移動
```

自動再投入ではなく手動確認フローとした理由は、障害の根本原因を確認せずに再処理すると同じ失敗が繰り返されるリスクがあるためである。

### Protobuf スキーマ管理

#### Schema Registry

Confluent Schema Registry を使用し、Protobuf スキーマのバージョン管理と互換性チェックを行う。

| 項目                | 設定                              |
| ------------------- | --------------------------------- |
| Schema Registry     | Confluent Schema Registry         |
| スキーマ形式        | Protobuf                          |
| 互換性モード        | BACKWARD（デフォルト）            |
| Subject 命名        | `{topic-name}-value`              |

#### Schema Registry デプロイ構成

Schema Registry は共通基盤として `k1s0-system` Namespace に配置し、Confluent Schema Registry の Helm Chart でデプロイする。Schema Registry を Kafka クラスタと同じ `messaging` Namespace ではなく `k1s0-system` に配置する理由: Schema Registry は全 Tier（system / business / service）のサービスから共通的に利用される基盤コンポーネントである。[kubernetes設計.md](../../infrastructure/kubernetes/kubernetes設計.md) の Namespace 設計で `k1s0-system` は「system tier のサーバー・DB・Schema Registry」を対象としており、Kafka ブローカー（messaging NS）とは異なるライフサイクル・アクセスポリシーで管理する。

| 項目              | 設定                                                              |
| ----------------- | ----------------------------------------------------------------- |
| Namespace         | `k1s0-system`                                                     |
| デプロイ方式      | Confluent Schema Registry Helm Chart (`cp-schema-registry`)       |
| レプリカ数 (prod) | 2（高可用性）                                                     |
| レプリカ数 (staging) | 1                                                              |

##### 環境別エンドポイント

| 環境          | エンドポイント                                                |
| ------------- | ------------------------------------------------------------- |
| ローカル開発（ホストマシンから）  | `localhost:8081`                                |
| ローカル開発（コンテナ間）        | `schema-registry:8081`                          |
| Kubernetes    | `schema-registry.k1s0-system.svc.cluster.local:8081`         |

> **Note**: ローカル開発環境では docker-compose.yaml に Schema Registry のサービス定義を追加する必要がある。詳細は [docker-compose設計.md](../../infrastructure/docker/docker-compose設計.md) を参照。

#### Buf によるスキーマ管理

Protobuf スキーマの lint と breaking change 検出を Buf で管理する。

```
api/proto/
├── k1s0/
│   ├── event/
│   │   ├── system/
│   │   │   ├── auth/
│   │   │   │   └── v1/
│   │   │   │       └── auth_events.proto
│   │   │   └── config/
│   │   │       └── v1/
│   │   │           └── config_events.proto
│   │   ├── business/
│   │   │   └── accounting/
│   │   │       └── v1/
│   │   │           └── accounting_events.proto
│   │   └── service/
│   │       ├── order/
│   │       │   └── v1/
│   │       │       └── order_events.proto
│   │       └── inventory/
│   │           └── v1/
│   │               └── inventory_events.proto
│   └── system/
│       └── common/
│           └── v1/
│               └── event_metadata.proto
└── buf.yaml
```

#### 共通イベントメタデータ

```protobuf
// k1s0/system/common/v1/event_metadata.proto
syntax = "proto3";
package k1s0.system.common.v1;

message EventMetadata {
  string event_id = 1;          // UUID
  string event_type = 2;        // e.g., "order.created"
  string source = 3;            // e.g., "order-server"
  int64 timestamp = 4;          // Unix timestamp (ms)
  string trace_id = 5;          // 分散トレース ID
  string correlation_id = 6;    // 業務相関 ID
  int32 schema_version = 7;     // スキーマバージョン
}
```

#### イベント定義例

```protobuf
// k1s0/event/service/order/v1/order_events.proto
syntax = "proto3";
package k1s0.event.service.order.v1;

import "k1s0/system/common/v1/event_metadata.proto";

message OrderCreatedEvent {
  k1s0.system.common.v1.EventMetadata metadata = 1;
  string order_id = 2;
  string customer_id = 3;
  repeated OrderItem items = 4;
  int64 total_amount = 5;       // 最小通貨単位（円）
  string currency = 6;
}

message OrderItem {
  string product_id = 1;
  int32 quantity = 2;
  int64 unit_price = 3;
}
```

---

## D-120: イベント駆動アーキテクチャ

### CQRS（Command Query Responsibility Segregation）

#### 適用方針

CQRS は **トラフィック量の非対称性が大きいサービス** に限定して適用する。読み取り / 書き込み比率が 10:1 を超える場合、または読み取りモデルの最適化が必要な場合に適用を検討する。単純な CRUD では適用しない。

```
Write Side:                      Read Side:
Command → Server → DB (Write)    Kafka → Projector → DB (Read)
                   ↓                                     ↓
                  Kafka Event                       Query API
```

| 適用基準                     | 適用判断 |
| ---------------------------- | -------- |
| 読み取り / 書き込み比率 > 10:1 | 適用検討 |
| 読み取りモデルの最適化が必要 | 適用検討 |
| 単純な CRUD                  | 適用しない |

### イベントソーシングの適用方針

イベントソーシングは **監査要件が厳しいドメイン** に限定して適用する。全変更履歴の保持が必要、監査証跡がコンプライアンス要件、状態復元・時点指定クエリが必要な場合に適用を検討する。通常の CRUD で十分な場合は適用しない。

| 適用基準                         | 適用判断   |
| -------------------------------- | ---------- |
| 全変更履歴の保持が必要           | 適用検討   |
| 監査証跡がコンプライアンス要件   | 適用検討   |
| 状態復元・時点指定クエリが必要   | 適用検討   |
| 通常の CRUD で十分               | 適用しない |

### Saga パターン

分散トランザクションには **コレオグラフィ方式** を基本とする。各サービスが自律的にイベントを発行・購読し、ビジネスフローを実現する。

#### コレオグラフィ方式

```
Order Service          Inventory Service        Payment Service
     │                        │                        │
     │  OrderCreated          │                        │
     │───────────────────────▶│                        │
     │                        │  InventoryReserved     │
     │                        │───────────────────────▶│
     │                        │                        │  PaymentCompleted
     │◀───────────────────────────────────────────────│
     │  OrderConfirmed        │                        │
     │                        │                        │
```

#### 補償トランザクション

処理失敗時は補償イベントを発行してロールバックする。

```
Order Service          Inventory Service        Payment Service
     │                        │                        │
     │  OrderCreated          │                        │
     │───────────────────────▶│                        │
     │                        │  InventoryReserved     │
     │                        │───────────────────────▶│
     │                        │                        │  PaymentFailed
     │                        │◀───────────────────────│
     │                        │  InventoryReleased     │
     │◀──────────────────────│                        │
     │  OrderCancelled        │                        │
     │                        │                        │
```

#### 方式の選定基準

| 方式               | 採用基準                  | メリット                | デメリット              |
| ------------------ | ------------------------- | ----------------------- | ----------------------- |
| コレオグラフィ     | ステップ数 < 5            | 疎結合、スケーラブル    | フロー全体の把握が困難  |
| オーケストレーション | ステップ数 >= 5          | フロー全体の可視性      | Orchestrator が SPOF    |

5 ステップ以上の複雑なワークフローでは、オーケストレーション方式（Saga Orchestrator）を検討する。

#### Saga Orchestrator 実装パターン

Orchestrator は **Rust で自前の軽量 Orchestrator** を構築する（Temporal 等の外部ワークフローエンジンは導入しない）。外部ワークフローエンジンを導入しない理由は、k1s0 のワークフローが比較的シンプルであり、Temporal 等の導入による運用コストが利点を上回ると判断したためである。詳細は [system-saga-server.md](../../servers/saga/server.md) を参照。

| 項目                   | 設計                                                       |
| ---------------------- | ---------------------------------------------------------- |
| 実装言語               | Rust                                                       |
| 配置パス               | `regions/system/server/rust/saga/`                         |
| ステート管理           | PostgreSQL（`saga_states` + `saga_step_logs` テーブル）    |
| ワークフロー定義       | YAML ベースのワークフロー定義ファイル                      |
| サービス呼び出し       | gRPC（各ステップの実行・補償トランザクションとも）         |
| デフォルトタイムアウト | 各ステップ個別に設定（デフォルト: 30 秒）                  |
| リトライ方式           | 指数バックオフ（最大 3 回）                                |
| 起動時リカバリ         | 未完了 Saga の自動検出・再開                               |

##### ステート管理テーブル

```sql
CREATE TABLE saga_states (
    saga_id       UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workflow_name VARCHAR(255) NOT NULL,
    current_step  INT NOT NULL DEFAULT 0,
    status        VARCHAR(50) NOT NULL DEFAULT 'STARTED',  -- STARTED, RUNNING, COMPLETED, COMPENSATING, FAILED
    payload       JSONB NOT NULL,
    created_at    TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at    TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE saga_step_logs (
    id            UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    saga_id       UUID NOT NULL REFERENCES saga_states(saga_id),
    step_index    INT NOT NULL,
    step_name     VARCHAR(255) NOT NULL,
    action        VARCHAR(50) NOT NULL,  -- EXECUTE, COMPENSATE
    status        VARCHAR(50) NOT NULL,  -- SUCCESS, FAILED, TIMEOUT
    request       JSONB,
    response      JSONB,
    error_message TEXT,
    executed_at   TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

ワークフロー定義例:

```yaml
# workflows/order-fulfillment.yaml
name: order-fulfillment
steps:
  - name: reserve-inventory
    service: inventory-service
    method: InventoryService.Reserve
    compensate: InventoryService.Release
    timeout_secs: 30
    retry:
      max_attempts: 3
      backoff: exponential
      initial_interval_ms: 1000

  - name: process-payment
    service: payment-service
    method: PaymentService.Charge
    compensate: PaymentService.Refund
    timeout_secs: 60
    retry:
      max_attempts: 2
      backoff: exponential
      initial_interval_ms: 2000

  - name: arrange-shipping
    service: shipping-service
    method: ShippingService.CreateShipment
    compensate: ShippingService.CancelShipment
    timeout_secs: 30
```

### イベントスキーマ設計（Protobuf）

#### スキーマ進化ルール

Protobuf のスキーマ進化は以下のルールに従う（Schema Registry の BACKWARD 互換性モードに準拠）。

| 変更種別           | 互換性 | 対応方法                     |
| ------------------ | ------ | ---------------------------- |
| フィールド追加     | 互換   | 新しいフィールド番号で追加   |
| フィールド削除     | 互換   | `reserved` で番号を予約      |
| フィールド型変更   | 非互換 | 新バージョンのトピックを作成 |
| フィールド名変更   | 互換   | ワイヤーフォーマットに影響なし |

### メッセージングライブラリ抽象 API

サーバーが Kafka を直接操作するのではなく、`messaging` ライブラリが提供する抽象 API を使用する。これにより、メッセージブローカーの実装詳細をサーバーコードから分離する。

- **EventProducer**: イベント発行の抽象インターフェース（Go: interface、Rust: trait）
- **EventConsumer**: イベント受信の抽象インターフェース
- **EventEnvelope**: トピック名・パーティションキー・ペイロード・ヘッダーを含むメッセージラッパー
- **EventMetadata**: イベント ID・種別・ソース・タイムスタンプ・トレース ID・相関 ID・スキーマバージョンを含む共通メタデータ

サーバーコードは `EventProducer.publish(envelope)` で EventEnvelope を発行する。Kafka への接続・シリアライゼーション・パーティショニングは `KafkaEventProducer`（Rust）や将来の Kafka 実装（Go）が担当する。

> **注記**: ライブラリの EventMetadata における `trace_id` と `correlation_id` は、Rust では `Option<String>` として実装している。Proto の string 型（デフォルト空文字列）とは異なるセマンティクスだが、呼び出し側での明示的な設定を促す設計判断として Option を採用している。

### Go Kafka Producer 実装参考例

> **注記**: 以下は Kafka Producer/Consumer の内部実装の参考例である。サーバーコードからは上記のメッセージングライブラリ抽象 API を使用する。

```go
// internal/infra/messaging/producer.go
package messaging

import (
    "context"

    "github.com/segmentio/kafka-go"
    "google.golang.org/protobuf/proto"
)

type EventProducer struct {
    writer *kafka.Writer
}

func NewEventProducer(brokers []string, topic string) *EventProducer {
    return &EventProducer{
        writer: &kafka.Writer{
            Addr:         kafka.TCP(brokers...),
            Topic:        topic,
            Balancer:     &kafka.Hash{},   // パーティションキーによる分散
            RequiredAcks: kafka.RequireAll, // acks=all
            Async:        false,
        },
    }
}

func (p *EventProducer) Publish(ctx context.Context, key string, event proto.Message) error {
    value, err := proto.Marshal(event)
    if err != nil {
        return fmt.Errorf("marshal event: %w", err)
    }

    return p.writer.WriteMessages(ctx, kafka.Message{
        Key:   []byte(key),
        Value: value,
    })
}
```

### Go Kafka Consumer 実装参考例

> **注記**: 以下は Kafka Producer/Consumer の内部実装の参考例である。サーバーコードからは上記のメッセージングライブラリ抽象 API を使用する。

```go
// internal/infra/messaging/consumer.go
package messaging

import (
    "context"
    "log/slog"

    "github.com/segmentio/kafka-go"
)

type EventConsumer struct {
    reader *kafka.Reader
    logger *slog.Logger
}

func NewEventConsumer(brokers []string, topic, groupID string, logger *slog.Logger) *EventConsumer {
    return &EventConsumer{
        reader: kafka.NewReader(kafka.ReaderConfig{
            Brokers:  brokers,
            Topic:    topic,
            GroupID:  groupID,
            MinBytes: 1e3,    // 1KB
            MaxBytes: 10e6,   // 10MB
        }),
        logger: logger,
    }
}

func (c *EventConsumer) Consume(ctx context.Context, handler func(context.Context, kafka.Message) error) error {
    for {
        msg, err := c.reader.ReadMessage(ctx)
        if err != nil {
            return fmt.Errorf("read message: %w", err)
        }

        if err := handler(ctx, msg); err != nil {
            c.logger.Error("failed to handle message",
                slog.String("topic", msg.Topic),
                slog.Int("partition", msg.Partition),
                slog.Int64("offset", msg.Offset),
                slog.String("error", err.Error()),
            )
            // DLQ への転送はリトライ後に実施
        }
    }
}
```

### Rust Kafka Producer 実装参考例

> **注記**: 以下は Kafka Producer/Consumer の内部実装の参考例である。サーバーコードからは上記のメッセージングライブラリ抽象 API を使用する。

```rust
// src/infra/messaging/producer.rs
use prost::Message;
use rdkafka::producer::{FutureProducer, FutureRecord};
use rdkafka::ClientConfig;
use std::time::Duration;

pub struct EventProducer {
    producer: FutureProducer,
    topic: String,
}

impl EventProducer {
    pub fn new(brokers: &str, topic: &str) -> Self {
        let producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("acks", "all")
            .set("message.timeout.ms", "5000")
            .create()
            .expect("Failed to create producer");

        Self {
            producer,
            topic: topic.to_string(),
        }
    }

    pub async fn publish<M: Message>(&self, key: &str, event: &M) -> Result<(), Box<dyn std::error::Error>> {
        let payload = event.encode_to_vec();

        self.producer
            .send(
                FutureRecord::to(&self.topic)
                    .key(key)
                    .payload(&payload),
                Duration::from_secs(5),
            )
            .await
            .map_err(|(e, _)| e)?;

        Ok(())
    }
}
```

### Rust Kafka Consumer 実装参考例

> **注記**: 以下は Kafka Producer/Consumer の内部実装の参考例である。サーバーコードからは上記のメッセージングライブラリ抽象 API を使用する。

```rust
// src/infra/messaging/consumer.rs
use rdkafka::consumer::{CommitMode, Consumer, StreamConsumer};
use rdkafka::{ClientConfig, Message};
use futures::StreamExt;

pub struct EventConsumer {
    consumer: StreamConsumer,
}

impl EventConsumer {
    pub fn new(brokers: &str, group_id: &str, topics: &[&str]) -> Self {
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("group.id", group_id)
            .set("auto.offset.reset", "earliest")
            .set("enable.auto.commit", "false")
            .create()
            .expect("Failed to create consumer");

        consumer.subscribe(topics).expect("Failed to subscribe");

        Self { consumer }
    }

    pub async fn consume<F, Fut>(&self, handler: F) -> Result<(), Box<dyn std::error::Error>>
    where
        F: Fn(Vec<u8>) -> Fut,
        Fut: std::future::Future<Output = Result<(), Box<dyn std::error::Error>>>,
    {
        let mut stream = self.consumer.stream();

        while let Some(result) = stream.next().await {
            let msg = result?;
            if let Some(payload) = msg.payload() {
                handler(payload.to_vec()).await?;
                self.consumer.commit_message(&msg, CommitMode::Async)?;
            }
        }

        Ok(())
    }
}
```

---

## 関連ドキュメント

- [system-saga-server.md](../../servers/saga/server.md) — Saga Orchestrator サーバー設計
- [tier-architecture.md](../overview/tier-architecture.md) — Tier アーキテクチャの詳細
- [API設計.md](../api/API設計.md) — REST API・gRPC・GraphQL 設計
- [kubernetes設計.md](../../infrastructure/kubernetes/kubernetes設計.md) — Namespace 設計（messaging NS）
- [可観測性設計.md](../observability/可観測性設計.md) — Kafka メトリクス・ログ設計
- [サービスメッシュ設計.md](../../infrastructure/service-mesh/サービスメッシュ設計.md) — サービス間通信の耐障害性
- [CI-CD設計.md](../../infrastructure/cicd/CI-CD設計.md) — Proto スキーマの CI チェック
- [config.md](../../cli/config/config設計.md) — config.yaml スキーマ（kafka セクション）
- [docker-compose設計.md](../../infrastructure/docker/docker-compose設計.md) — ローカル開発環境の Kafka/Schema Registry 設定
- [認証認可設計.md](../auth/認証認可設計.md) — 認証認可・シークレット管理
- [helm設計.md](../../infrastructure/kubernetes/helm設計.md) — Helm Chart 設計
- [テンプレート仕様-サーバー.md](../../templates/server/サーバー.md) — サーバーテンプレート（Kafka Producer/Consumer）
