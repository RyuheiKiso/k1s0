# 可観測性設計

> **ガイド**: 設計背景・選定理由・初期化パターンは [可観測性設計.guide.md](./可観測性設計.guide.md) を参照。

k1s0 における監視・アラート・SLO/SLA・構造化ログの仕様を定義する。
Tier アーキテクチャの詳細は [tier-architecture.md](../../architecture/overview/tier-architecture.md) を参照。

## 基本方針

- **メトリクス**: Prometheus で収集し、Grafana で可視化する
- **ログ**: JSON 構造化ログを標準とし、Loki で集約する
- **トレース**: OpenTelemetry + Jaeger で分散トレーシングを実現する
- **アラート**: Alertmanager から Microsoft Teams へ通知する
- すべてのコンポーネントは `observability` Namespace にデプロイする

---

## 詳細設計ドキュメント

各設計の詳細は以下の分割ドキュメントを参照。

| ドキュメント | 内容 |
| --- | --- |
| [可観測性-監視アラート設計.md](./監視アラート設計.md) | D-107: RED/USE メトリクス、Prometheus、Grafana、Alertmanager、Tier 別アラートルール |
| [可観測性-SLO設計.md](./SLO設計.md) | D-108: SLO/SLA 定義、SLI、エラーバジェット運用 |
| [可観測性-トレーシング設計.md](./トレーシング設計.md) | D-110: 分散トレーシング、OpenTelemetry SDK 初期化パターン |
| [可観測性-ログ設計.md](./ログ設計.md) | D-109: 構造化ログ、JSON 標準フィールド、Loki・Promtail |

---

## ローカル開発環境デプロイ手順

docker-compose の `observability` プロファイルで可観測性スタック（Jaeger, Prometheus, Grafana, Loki）をローカルにデプロイする。

### 起動方法

```bash
# インフラ + 可観測性スタックを起動
docker compose --profile infra --profile observability up -d

# 可観測性スタックのみ起動（DB 等は不要な場合）
docker compose --profile observability up -d
```

### コンポーネントとポート一覧

| コンポーネント | ホストポート | 用途                                |
| -------------- | ------------ | ----------------------------------- |
| Jaeger UI      | 16686        | 分散トレースの検索・可視化          |
| Jaeger OTLP    | 4317 (gRPC)  | OpenTelemetry トレースの受信        |
| Jaeger OTLP    | 4318 (HTTP)  | OpenTelemetry トレースの受信        |
| Prometheus     | 9090         | メトリクスの収集・クエリ            |
| Grafana        | 3200         | ダッシュボード（ログ・メトリクス・トレース統合表示） |
| Loki           | 3100         | ログの集約・クエリ                  |

初回セットアップ手順・ポート選定理由・Promtail 省略の背景はガイドを参照。

---

## Prometheus 設定（ローカル開発環境）

ローカル開発環境用の Prometheus 設定を `infra/docker/prometheus/prometheus.yaml` に配置する。

### scrape_configs

```yaml
# infra/docker/prometheus/prometheus.yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - /etc/prometheus/recording_rules.yaml
  - /etc/prometheus/alerting_rules.yaml

scrape_configs:
  # Prometheus 自身
  - job_name: prometheus
    static_configs:
      - targets: ["localhost:9090"]

  # auth-server (Rust)
  - job_name: auth-server-rust
    static_configs:
      - targets: ["auth-rust:8080"]
        labels:
          service: auth-server
          tier: system
          lang: rust
    metrics_path: /metrics

  # config-server (Rust)
  - job_name: config-server-rust
    static_configs:
      - targets: ["config-rust:8082"]
        labels:
          service: config-server
          tier: system
          lang: rust
    metrics_path: /metrics
```

### Recording Rules

RED メトリクスの事前集計ルール。

```yaml
# infra/docker/prometheus/recording_rules.yaml
groups:
  - name: red-recording-rules
    interval: 30s
    rules:
      # リクエストレート（サービス別）
      - record: service:http_requests:rate5m
        expr: sum(rate(http_requests_total[5m])) by (service)

      # エラーレート（サービス別）
      - record: service:http_errors:rate5m
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
          / sum(rate(http_requests_total[5m])) by (service)

      # P99 レイテンシ（サービス別）
      - record: service:http_request_duration_seconds:p99
        expr: |
          histogram_quantile(0.99,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          )

      # P95 レイテンシ（サービス別）
      - record: service:http_request_duration_seconds:p95
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          )

      # gRPC リクエストレート（サービス別）
      - record: service:grpc_requests:rate5m
        expr: sum(rate(grpc_server_handled_total[5m])) by (service, grpc_service)

      # gRPC エラーレート（サービス別）
      - record: service:grpc_errors:rate5m
        expr: |
          sum(rate(grpc_server_handled_total{grpc_code!="OK"}[5m])) by (service, grpc_service)
          / sum(rate(grpc_server_handled_total[5m])) by (service, grpc_service)

      # gRPC P99 レイテンシ
      - record: service:grpc_handling_seconds:p99
        expr: |
          histogram_quantile(0.99,
            sum(rate(grpc_server_handling_seconds_bucket[5m])) by (service, grpc_service, le)
          )

  - name: slo-recording-rules
    rules:
      - record: slo:availability:ratio
        expr: |
          sum(rate(http_requests_total{status!~"5.."}[30d])) by (namespace, service)
          / sum(rate(http_requests_total[30d])) by (namespace, service)

      - record: slo:error_budget:remaining
        expr: |
          1 - (
            (1 - slo:availability:ratio)
            / (1 - (
              label_replace(
                vector(0.9995) and on() (kube_namespace_labels{namespace="k1s0-system"})
                or vector(0.999),
                "namespace", "$1", "namespace", "(.*)"
              )
            ))
          )
```

### Alerting Rules（ローカル開発用）

```yaml
# infra/docker/prometheus/alerting_rules.yaml
groups:
  - name: local-dev-alerts
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "{{ $labels.job }} is down"
          description: "{{ $labels.instance }} has been unreachable for more than 1 minute"

      - alert: HighErrorRate
        expr: service:http_errors:rate5m > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "{{ $labels.service }} error rate > 5%"
          description: "{{ $labels.service }} の 5xx エラー率が {{ $value | humanizePercentage }} です"

      - alert: HighLatency
        expr: service:http_request_duration_seconds:p99 > 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "{{ $labels.service }} P99 latency > 1s"
```

---

## Grafana ダッシュボード設定（ローカル開発環境）

### プロビジョニング設定

Grafana の自動設定ファイルを `infra/docker/grafana/provisioning/` に配置する。

#### データソース設定

```yaml
# infra/docker/grafana/provisioning/datasources/datasources.yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false

  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    editable: false
    jsonData:
      derivedFields:
        - datasourceUid: jaeger
          matcherRegex: '"trace_id":"([a-f0-9]+)"'
          name: TraceID
          url: '$${__value.raw}'

  - name: Jaeger
    type: jaeger
    access: proxy
    uid: jaeger
    url: http://jaeger:16686
    editable: false
```

#### ダッシュボードプロビジョニング

```yaml
# infra/docker/grafana/provisioning/dashboards/dashboards.yaml
apiVersion: 1

providers:
  - name: k1s0
    orgId: 1
    folder: k1s0
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /var/lib/grafana/dashboards
      foldersFromFilesStructure: false
```

### System Tier Overview ダッシュボード

`infra/docker/grafana/dashboards/system-overview.json` に配置する。以下は主要パネルの PromQL 定義。

#### リクエスト率・エラー率・レイテンシ（RED）

| パネル名              | PromQL                                                                                              | 可視化タイプ  |
| --------------------- | --------------------------------------------------------------------------------------------------- | ------------- |
| リクエスト率          | `sum(rate(http_requests_total{tier="system"}[5m])) by (service)`                                    | Time Series   |
| エラーレート          | `service:http_errors:rate5m{service=~"auth-server\|config-server"}`                                  | Time Series   |
| P50 レイテンシ        | `histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{tier="system"}[5m])) by (service, le))` | Time Series   |
| P95 レイテンシ        | `service:http_request_duration_seconds:p95{service=~"auth-server\|config-server"}`                   | Time Series   |
| P99 レイテンシ        | `service:http_request_duration_seconds:p99{service=~"auth-server\|config-server"}`                   | Time Series   |

#### gRPC メトリクス

| パネル名              | PromQL                                                                                              | 可視化タイプ  |
| --------------------- | --------------------------------------------------------------------------------------------------- | ------------- |
| gRPC リクエスト率     | `sum(rate(grpc_server_handled_total{tier="system"}[5m])) by (service, grpc_service)`                | Time Series   |
| gRPC エラーレート     | `service:grpc_errors:rate5m{service=~"auth-server\|config-server"}`                                  | Time Series   |
| gRPC P99 レイテンシ   | `service:grpc_handling_seconds:p99{service=~"auth-server\|config-server"}`                            | Time Series   |
| gRPC ステータスコード | `sum(rate(grpc_server_handled_total{tier="system"}[5m])) by (service, grpc_code)`                   | Bar Chart     |

#### DB クエリ時間

| パネル名              | PromQL                                                                                              | 可視化タイプ  |
| --------------------- | --------------------------------------------------------------------------------------------------- | ------------- |
| DB クエリ P99         | `histogram_quantile(0.99, sum(rate(db_query_duration_seconds_bucket[5m])) by (query_name, le))`     | Time Series   |
| DB クエリ数           | `sum(rate(db_query_duration_seconds_count[5m])) by (query_name)`                                    | Time Series   |

#### Kafka メッセージ数

| パネル名              | PromQL                                                                                              | 可視化タイプ  |
| --------------------- | --------------------------------------------------------------------------------------------------- | ------------- |
| メッセージ生成レート  | `sum(rate(kafka_messages_produced_total[5m])) by (service, topic)`                                  | Time Series   |
| メッセージ消費レート  | `sum(rate(kafka_messages_consumed_total[5m])) by (service, topic, consumer_group)`                  | Time Series   |

---

## Loki + Promtail 設定

### ローカル開発環境（docker-compose）

ローカルでは Loki のみ起動し、Promtail は省略する。アプリケーションのログは `docker compose logs` で直接確認する。

#### Loki 設定

```yaml
# infra/docker/loki/loki-config.yaml
auth_enabled: false

server:
  http_listen_port: 3100

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

schema_config:
  configs:
    - from: 2024-01-01
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h

limits_config:
  retention_period: 168h  # 7日（ローカル開発用）

storage_config:
  filesystem:
    directory: /loki/storage
```

#### ログラベル設計

| ラベル        | 説明                               | 例                       |
| ------------- | ---------------------------------- | ------------------------ |
| `service`     | サービス名                         | `auth-server`            |
| `tier`        | Tier 階層                          | `system`                 |
| `environment` | 環境名                             | `dev`                    |
| `level`       | ログレベル                         | `info`, `error`          |
| `trace_id`    | 分散トレースの Trace ID            | `abc123def456`           |

### Kubernetes 環境（Promtail）

Kubernetes 環境では Promtail が DaemonSet として各ノードにデプロイされ、Pod の stdout/stderr を収集する。設定は [可観測性-ログ設計.md](./ログ設計.md) の「ログ集約（Loki）」セクションを参照。

---

## Jaeger 分散トレース設定

### ローカル開発環境

ローカルでは `jaegertracing/all-in-one` で Collector・Query・UI を単一コンテナ提供する。

### OpenTelemetry Collector 設定（Kubernetes 環境向け）

Kubernetes 環境では OpenTelemetry Collector でサンプリング・バッチ処理を集中管理する。

```yaml
# otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 5s
    send_batch_size: 512
  # テールベースサンプリング: エラーが含まれるトレースは必ず保持
  tail_sampling:
    decision_wait: 10s
    policies:
      - name: error-policy
        type: status_code
        status_code:
          status_codes: [ERROR]
      - name: rate-limiting
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

exporters:
  otlp/jaeger:
    endpoint: jaeger-collector:4317
    tls:
      insecure: true
  prometheus:
    endpoint: 0.0.0.0:8889
    namespace: otel

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch, tail_sampling]
      exporters: [otlp/jaeger]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [prometheus]
```

### トレースサンプリング戦略

| 環境    | サンプリング方式              | サンプリング率 |
| ------- | ----------------------------- | -------------- |
| dev     | AlwaysOn                      | 100%           |
| staging | TraceIDRatioBased             | 50%            |
| prod    | Tail-based (OTel Collector)   | 10% + エラー 100% |

サービスマップ・サンプリング戦略の設計背景はガイドを参照。

---

## アラート設定（ローカル開発環境）

### 通知先設計

| 環境    | 通知先                          | 方式                                           |
| ------- | ------------------------------- | ---------------------------------------------- |
| dev     | Prometheus UI で確認            | Alertmanager 不要                              |
| staging | Slack/Teams Webhook (dev チャネル) | Alertmanager → prometheus-msteams           |
| prod    | Teams Webhook (運用チャネル)    | Alertmanager → prometheus-msteams              |

### アラートルール一覧

| アラート名                     | 条件                                                    | severity | 対象 Tier            |
| ------------------------------ | ------------------------------------------------------- | -------- | -------------------- |
| SystemServiceErrorRateWarning  | system Tier 5xx エラー率 > 0.1%（5分間）                | warning  | system               |
| SystemServiceHighErrorRate     | system Tier 5xx エラー率 > 1%（5分間）                  | critical | system               |
| SystemServiceHighLatency       | system Tier P99 レイテンシ > 500ms（5分間）             | warning  | system               |
| ServiceErrorRateWarning        | business/service Tier 5xx エラー率 > 0.2%（5分間）      | warning  | business / service   |
| ServiceHighErrorRate           | business/service Tier 5xx エラー率 > 5%（5分間）        | critical | business / service   |
| ServiceHighLatency             | business/service Tier P99 レイテンシ > 1s（5分間）      | warning  | business / service   |
| ServiceDown（ローカル用）      | ターゲットが 1 分間 DOWN                                | warning  | 全 Tier              |
| HighErrorRate（ローカル用）    | エラー率 > 5%（2分間）                                  | warning  | 全 Tier              |
| HighLatency（ローカル用）      | P99 レイテンシ > 1s（2分間）                            | warning  | 全 Tier              |

---

## テレメトリライブラリとの接続設定

各言語のテレメトリライブラリは、OpenTelemetry SDK によるトレース送信と Prometheus メトリクスの公開を統一的に行う。

### 環境変数

`config.yaml` の `observability` セクションが優先。環境変数はフォールバック（詳細はガイドを参照）。

| 環境変数                       | 説明                              | dev 環境デフォルト値      |
| ------------------------------ | --------------------------------- | ------------------------- |
| `OTEL_EXPORTER_OTLP_ENDPOINT` | OTLP エクスポータのエンドポイント | `http://jaeger:4317`      |
| `OTEL_SERVICE_NAME`           | サービス名                        | config.yaml の `app.name` |
| `OTEL_TRACES_SAMPLER`         | サンプラー種別                    | `always_on`               |
| `OTEL_TRACES_SAMPLER_ARG`     | サンプラー引数（ratio 等）        | `1.0`                     |

### config.yaml の observability セクション

```yaml
# config/config.dev.yaml（ローカル開発用）
observability:
  log:
    level: "debug"
    format: "json"      # dev 環境では "text" も許容
  trace:
    enabled: true
    endpoint: "jaeger:4317"    # docker-compose サービス名
    sample_rate: 1.0           # dev: 100%
  metrics:
    enabled: true
    path: "/metrics"
```

```yaml
# config/config.yaml（本番用デフォルト）
observability:
  log:
    level: "warn"
    format: "json"
  trace:
    enabled: true
    endpoint: "jaeger.observability.svc.cluster.local:4317"
    sample_rate: 0.1       # prod: 10%（Collector でテールベースサンプリング）
  metrics:
    enabled: true
    path: "/metrics"
```

### 言語別テレメトリライブラリの初期化

Go / Rust / TypeScript / Dart の初期化コードはガイドを参照。

---

## カスタムメトリクス一覧

全サーバーが `/metrics` エンドポイントで公開するメトリクスの一覧。テレメトリライブラリ（Go / Rust / TypeScript / Dart）が提供する共通メトリクスと、サーバー固有のカスタムメトリクスに分類する。

### 共通メトリクス（テレメトリライブラリ提供）

| メトリクス名                      | 型        | ラベル                                      | 説明                                       |
| --------------------------------- | --------- | ------------------------------------------- | ------------------------------------------ |
| `http_requests_total`             | counter   | `service`, `method`, `path`, `status`       | HTTP リクエスト総数                        |
| `http_request_duration_seconds`   | histogram | `service`, `method`, `path`                 | HTTP リクエストのレイテンシ（バケット: 5ms〜10s） |
| `grpc_server_handled_total`       | counter   | `service`, `grpc_service`, `grpc_method`, `grpc_code` | gRPC リクエスト完了数                |
| `grpc_server_handling_seconds`    | histogram | `service`, `grpc_service`, `grpc_method`    | gRPC リクエストのレイテンシ（バケット: 5ms〜10s） |

ヒストグラムバケット境界: `[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]`（秒）

### サーバー固有メトリクス

| メトリクス名                      | 型        | ラベル                          | サーバー         | 説明                                |
| --------------------------------- | --------- | ------------------------------- | ---------------- | ----------------------------------- |
| `db_query_duration_seconds`       | histogram | `query_name`, `table`           | auth / config    | データベースクエリの実行時間        |
| `kafka_messages_produced_total`   | counter   | `topic`                         | config           | Kafka メッセージの送信数            |
| `kafka_messages_consumed_total`   | counter   | `topic`, `consumer_group`       | config           | Kafka メッセージの受信数            |

### メトリクス出力対応状況

| 言語       | ライブラリパス                                        | 出力方式                  |
| ---------- | ----------------------------------------------------- | ------------------------- |
| Go         | `regions/system/library/go/telemetry/metrics.go`      | `prometheus/client_golang` → `promhttp.Handler()` |
| Rust       | `regions/system/library/rust/telemetry/src/metrics.rs` | `prometheus` crate → `TextEncoder` |
| TypeScript | `regions/system/library/typescript/telemetry/src/metrics.ts` | 独自実装 → Prometheus テキストフォーマット |
| Dart       | `regions/system/library/dart/telemetry/lib/src/metrics.dart` | 独自実装 → Prometheus テキストフォーマット |

---

## 関連ドキュメント

- [可観測性-監視アラート設計.md](./監視アラート設計.md) -- D-107: 監視・アラート設計
- [可観測性-SLO設計.md](./SLO設計.md) -- D-108: SLO/SLA 定義
- [可観測性-トレーシング設計.md](./トレーシング設計.md) -- D-110: 分散トレーシング
- [可観測性-ログ設計.md](./ログ設計.md) -- D-109: 構造化ログ
- [tier-architecture.md](../../architecture/overview/tier-architecture.md) -- Tier アーキテクチャの詳細
- [kubernetes設計.md](../../infrastructure/kubernetes/kubernetes設計.md) -- Namespace・NetworkPolicy 設計
- [インフラ設計.md](../../infrastructure/overview/インフラ設計.md) -- オンプレミスインフラ構成
- [サービスメッシュ設計.md](../../infrastructure/service-mesh/サービスメッシュ設計.md) -- Istio 設計・耐障害性
- [config.md](../../cli/config/config設計.md) -- config.yaml スキーマと環境別管理
- [CI-CD設計.md](../../infrastructure/cicd/CI-CD設計.md) -- CI/CD パイプライン設計
- [helm設計.md](../../infrastructure/kubernetes/helm設計.md) -- Helm Chart 設計
- [メッセージング設計.md](../../architecture/messaging/メッセージング設計.md) -- Kafka メッセージング設計
- [APIゲートウェイ設計.md](../api/APIゲートウェイ設計.md) -- API Gateway 設計
- [docker-compose設計.md](../../infrastructure/docker/docker-compose設計.md) -- ローカル開発環境の可観測性サービス設定
- [テンプレート仕様-サーバー.md](../../templates/server/サーバー.md) -- サーバーテンプレート（OTel 初期化・構造化ログ）
