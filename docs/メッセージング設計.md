# メッセージング設計

Kafka を中心としたメッセージングおよびイベント駆動アーキテクチャの設計を定義する。
Tier アーキテクチャの詳細は [tier-architecture.md](tier-architecture.md) を参照。

## 基本方針

- メッセージブローカーは **Apache Kafka** を採用する
- メッセージスキーマは **Protobuf** で定義し、Schema Registry で管理する
- イベント駆動アーキテクチャで CQRS・Saga パターンを適用する
- Kafka クラスタは `messaging` Namespace にデプロイする

---

## D-119: Kafka トピック設計

### トピック命名規則

```
k1s0.{tier}.{domain}.{event-type}.{version}
```

| 要素           | 説明                          | 例                        |
| -------------- | ----------------------------- | ------------------------- |
| `k1s0`         | プロジェクトプレフィックス    | 固定                      |
| `tier`         | Tier 名                       | `system`, `business`, `service` |
| `domain`       | ドメイン名                    | `auth`, `order`, `inventory` |
| `event-type`   | イベント種別                  | `created`, `updated`, `command` |
| `version`      | スキーマバージョン            | `v1`, `v2`                |

#### トピック命名例

| トピック名                              | 用途                     |
| --------------------------------------- | ------------------------ |
| `k1s0.service.order.created.v1`         | 注文作成イベント         |
| `k1s0.service.order.updated.v1`         | 注文更新イベント         |
| `k1s0.service.inventory.reserved.v1`    | 在庫引当イベント         |
| `k1s0.business.accounting.entry.v1`     | 会計仕訳イベント         |
| `k1s0.system.auth.login.v1`            | ログインイベント         |

### パーティション戦略

| Tier     | デフォルトパーティション数 | レプリケーションファクター（prod） |
| -------- | -------------------------- | ---------------------------------- |
| system   | 6                          | 3                                  |
| business | 3                          | 3                                  |
| service  | 3                          | 3                                  |

> **注記**: レプリケーションファクターは prod 環境（3ブローカー）の値。staging/dev 環境ではブローカー数に応じて自動調整される（詳細は後述の「レプリケーション設定」を参照）。

パーティションキーの選定基準:
- **注文系**: `order_id`（同一注文のイベントを順序保証）
- **ユーザー系**: `user_id`（同一ユーザーの操作を順序保証）
- **汎用**: ドメインのエンティティ ID

### Kafka クラスタ構成（Strimzi Kafka Operator）

Kafka クラスタの管理には **Strimzi Kafka Operator** を採用する。

**採用理由:**
- Kubernetes ネイティブな Kafka 管理が可能（CRD ベースの宣言的管理）
- ローリングアップデートによる無停止アップグレードをサポート
- Kafka クラスタのスケーリング・監視・セキュリティを Operator が自動管理

| 項目         | 設定                                    |
| ------------ | --------------------------------------- |
| Operator     | Strimzi Kafka Operator                  |
| Namespace    | `messaging`                             |
| Helm Chart   | `strimzi-kafka-operator` (strimzi.io)   |

#### 環境別クラスタ構成

| 環境                 | Kafka ブローカー | ZooKeeper ノード | 備考                                                    |
| -------------------- | ---------------- | ---------------- | ------------------------------------------------------- |
| prod                 | 3                | 3                | KRaft モードへの移行は Strimzi の対応状況を見て検討     |
| staging              | 1                | 1                | 最小構成でコスト最適化                                  |
| dev (docker-compose) | 1                | なし             | KRaft モード（ZooKeeper 不要）で単体起動                |

### レプリケーション設定

```
min.insync.replicas = 2
acks = all
```

| 環境    | ブローカー数 | レプリケーションファクター | min.insync.replicas |
| ------- | ------------ | -------------------------- | ------------------- |
| dev     | 1            | 1                          | 1                   |
| staging | 1            | 1                          | 1                   |
| prod    | 3            | 3                          | 2                   |

### 消費者グループ命名

```
{service-name}.{purpose}
```

| 消費者グループ名                  | サービス       | 用途                     |
| --------------------------------- | -------------- | ------------------------ |
| `order-server.inventory-check`    | order-server   | 在庫確認処理             |
| `inventory-server.order-reserved` | inventory-server | 注文引当処理           |
| `notification-server.order-event` | notification-server | 注文通知            |
| `analytics-server.all-events`     | analytics-server    | 分析用イベント集約  |

### トピック保持ポリシー

| カテゴリ       | 保持期間  | クリーンアップポリシー | 備考                     |
| -------------- | --------- | ---------------------- | ------------------------ |
| ビジネスイベント | 7 日    | delete                 | 通常のイベント           |
| 監査ログ（Kafka トピック） | 90 日 | delete          | コンプライアンス要件（アプリケーション監査ログの長期保存は可観測性設計.md を参照） |
| コンパクト     | 無期限    | compact                | 最新状態の保持           |
| DLQ            | 30 日     | delete                 | 処理失敗メッセージ       |

### DLQ（Dead Letter Queue）設計

処理に失敗したメッセージは DLQ トピックに転送する。

```
k1s0.{tier}.{domain}.{event-type}.{version}.dlq
```

| 項目           | 設定                                    |
| -------------- | --------------------------------------- |
| DLQ トピック名 | 元トピック名 + `.dlq` サフィックス      |
| リトライ回数   | 3 回（Exponential Backoff）             |
| DLQ 保持期間   | 30 日                                   |
| アラート       | DLQ にメッセージ到達時に Teams 通知     |

```
Consumer → 処理失敗 → リトライ (3回) → DLQ トピック → アラート通知
```

#### DLQ メッセージ再処理方式

DLQ に滞留したメッセージの再処理は、管理 API エンドポイント経由で行う。

| 項目                 | 設計                                                              |
| -------------------- | ----------------------------------------------------------------- |
| メッセージ確認       | `GET /admin/dlq/{topic}/messages` — DLQ 内メッセージの一覧・内容確認 |
| 再処理実行           | `POST /admin/dlq/{topic}/replay` — DLQ から元トピックへ再投入    |
| 再処理回数上限       | 3 回（超過分はアーカイブ用トピック `*.dlq.archive` に移動）       |
| 監視                 | Grafana ダッシュボードで DLQ メッセージ数を監視し、アラート設定   |

##### 再処理フロー

```
1. DLQ メッセージ発生 → Grafana アラート通知
2. GET /admin/dlq/{topic}/messages でメッセージ内容を確認
3. 問題解消後、POST /admin/dlq/{topic}/replay で再処理を実行
4. DLQ トピックからメッセージを読み取り → 元のトピックに再投入
5. 再処理回数が上限（3回）を超えた場合 → *.dlq.archive トピックに移動
```

### Protobuf スキーマ管理

#### Schema Registry

Confluent Schema Registry を使用し、Protobuf スキーマのバージョン管理と互換性チェックを行う。

| 項目                | 設定                              |
| ------------------- | --------------------------------- |
| Schema Registry     | Confluent Schema Registry         |
| スキーマ形式        | Protobuf                          |
| 互換性モード        | BACKWARD（デフォルト）            |
| Subject 命名        | `{topic-name}-value`              |

#### Schema Registry デプロイ構成

Schema Registry は共通基盤として `k1s0-system` Namespace に配置し、Confluent Schema Registry の Helm Chart でデプロイする。

> **Note**: Schema Registry を Kafka クラスタと同じ `messaging` Namespace ではなく `k1s0-system` に配置する理由:
> Schema Registry は全 Tier（system / business / service）のサービスから共通的に利用される基盤コンポーネントである。[kubernetes設計.md](kubernetes設計.md) の Namespace 設計で `k1s0-system` は「system 層のサーバー・DB・Schema Registry」を対象としており、Kafka ブローカー（messaging NS）とは異なるライフサイクル・アクセスポリシーで管理する。

| 項目              | 設定                                                              |
| ----------------- | ----------------------------------------------------------------- |
| Namespace         | `k1s0-system`                                                     |
| デプロイ方式      | Confluent Schema Registry Helm Chart (`cp-schema-registry`)       |
| レプリカ数 (prod) | 2（高可用性）                                                     |
| レプリカ数 (staging) | 1                                                              |

##### 環境別エンドポイント

| 環境          | エンドポイント                                                |
| ------------- | ------------------------------------------------------------- |
| ローカル開発  | `localhost:8081`                                              |
| Kubernetes    | `schema-registry.k1s0-system.svc.cluster.local:8081`         |

> **Note**: ローカル開発環境では docker-compose.yaml に Schema Registry のサービス定義を追加する必要がある。詳細は [docker-compose設計.md](docker-compose設計.md) を参照。

#### Buf によるスキーマ管理

Protobuf スキーマの lint と breaking change 検出を Buf で管理する。

```
api/proto/
├── k1s0/
│   ├── event/
│   │   ├── system/
│   │   │   └── auth/
│   │   │       └── v1/
│   │   │           └── login_event.proto
│   │   ├── business/
│   │   │   └── accounting/
│   │   │       └── v1/
│   │   │           └── entry_event.proto
│   │   └── service/
│   │       ├── order/
│   │       │   └── v1/
│   │       │       └── order_event.proto
│   │       └── inventory/
│   │           └── v1/
│   │               └── inventory_event.proto
│   └── system/
│       └── common/
│           └── v1/
│               └── event_metadata.proto
└── buf.yaml
```

#### 共通イベントメタデータ

```protobuf
// k1s0/system/common/v1/event_metadata.proto
syntax = "proto3";
package k1s0.system.common.v1;

message EventMetadata {
  string event_id = 1;          // UUID
  string event_type = 2;        // e.g., "order.created"
  string source = 3;            // e.g., "order-server"
  int64 timestamp = 4;          // Unix timestamp (ms)
  string trace_id = 5;          // 分散トレース ID
  string correlation_id = 6;    // 業務相関 ID
  int32 schema_version = 7;     // スキーマバージョン
}
```

#### イベント定義例

```protobuf
// k1s0/event/service/order/v1/order_event.proto
syntax = "proto3";
package k1s0.event.service.order.v1;

import "k1s0/system/common/v1/event_metadata.proto";

message OrderCreatedEvent {
  k1s0.system.common.v1.EventMetadata metadata = 1;
  string order_id = 2;
  string customer_id = 3;
  repeated OrderItem items = 4;
  int64 total_amount = 5;       // 最小通貨単位（円）
  string currency = 6;
}

message OrderItem {
  string product_id = 1;
  int32 quantity = 2;
  int64 unit_price = 3;
}
```

---

## D-120: イベント駆動アーキテクチャ

### CQRS（Command Query Responsibility Segregation）

#### 適用方針

CQRS は **トラフィック量の非対称性が大きいサービス** に限定して適用する。

| 適用基準                     | 適用判断 |
| ---------------------------- | -------- |
| 読み取り / 書き込み比率 > 10:1 | 適用検討 |
| 読み取りモデルの最適化が必要 | 適用検討 |
| 単純な CRUD                  | 適用しない |

#### CQRS アーキテクチャ

```
Write Side:                      Read Side:
Command → Server → DB (Write)    Kafka → Projector → DB (Read)
                   ↓                                     ↓
                  Kafka Event                       Query API
```

### イベントソーシングの適用方針

イベントソーシングは **監査要件が厳しいドメイン** に限定して適用する。

| 適用基準                         | 適用判断   |
| -------------------------------- | ---------- |
| 全変更履歴の保持が必要           | 適用検討   |
| 監査証跡がコンプライアンス要件   | 適用検討   |
| 状態復元・時点指定クエリが必要   | 適用検討   |
| 通常の CRUD で十分               | 適用しない |

### Saga パターン

分散トランザクションには **コレオグラフィ方式** を基本とする。

#### コレオグラフィ方式

各サービスが自律的にイベントを発行・購読し、ビジネスフローを実現する。

```
Order Service          Inventory Service        Payment Service
     │                        │                        │
     │  OrderCreated          │                        │
     │───────────────────────▶│                        │
     │                        │  InventoryReserved     │
     │                        │───────────────────────▶│
     │                        │                        │  PaymentCompleted
     │◀───────────────────────────────────────────────│
     │  OrderConfirmed        │                        │
     │                        │                        │
```

#### 補償トランザクション

処理失敗時は補償イベントを発行してロールバックする。

```
Order Service          Inventory Service        Payment Service
     │                        │                        │
     │  OrderCreated          │                        │
     │───────────────────────▶│                        │
     │                        │  InventoryReserved     │
     │                        │───────────────────────▶│
     │                        │                        │  PaymentFailed
     │                        │◀───────────────────────│
     │                        │  InventoryReleased     │
     │◀──────────────────────│                        │
     │  OrderCancelled        │                        │
     │                        │                        │
```

#### オーケストレーション方式の採用基準

5 ステップ以上の複雑なワークフローでは、オーケストレーション方式（Saga Orchestrator）を検討する。

| 方式               | 採用基準                  | メリット                | デメリット              |
| ------------------ | ------------------------- | ----------------------- | ----------------------- |
| コレオグラフィ     | ステップ数 < 5            | 疎結合、スケーラブル    | フロー全体の把握が困難  |
| オーケストレーション | ステップ数 >= 5          | フロー全体の可視性      | Orchestrator が SPOF    |

#### Saga Orchestrator 実装パターン

Orchestrator は **Go で自前の軽量 Orchestrator** を構築する（Temporal 等の外部ワークフローエンジンは導入しない）。

| 項目                   | 設計                                                       |
| ---------------------- | ---------------------------------------------------------- |
| 実装言語               | Go                                                         |
| ステート管理           | PostgreSQL（`saga_states` テーブル）                       |
| ワークフロー定義       | JSON/YAML ベースのワークフロー定義ファイル                 |
| サービス呼び出し       | gRPC（各ステップの実行・補償トランザクションとも）         |
| デフォルトタイムアウト | 各ステップ個別に設定（デフォルト: 30 秒）                  |
| リトライ方式           | 指数バックオフ（最大 3 回）                                |

##### ステート管理テーブル

```sql
CREATE TABLE saga_states (
    saga_id       UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workflow_name VARCHAR(255) NOT NULL,
    current_step  INT NOT NULL DEFAULT 0,
    status        VARCHAR(50) NOT NULL DEFAULT 'STARTED',  -- STARTED, RUNNING, COMPLETED, COMPENSATING, FAILED
    payload       JSONB NOT NULL,
    created_at    TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at    TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE saga_step_logs (
    id            UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    saga_id       UUID NOT NULL REFERENCES saga_states(saga_id),
    step_index    INT NOT NULL,
    step_name     VARCHAR(255) NOT NULL,
    action        VARCHAR(50) NOT NULL,  -- EXECUTE, COMPENSATE
    status        VARCHAR(50) NOT NULL,  -- SUCCESS, FAILED, TIMEOUT
    request       JSONB,
    response      JSONB,
    error_message TEXT,
    executed_at   TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

##### ワークフロー定義例

```yaml
# workflows/order-fulfillment.yaml
name: order-fulfillment
steps:
  - name: reserve-inventory
    service: inventory-server
    method: InventoryService.Reserve
    compensate: InventoryService.Release
    timeout: 30s
    retry:
      max_attempts: 3
      backoff: exponential

  - name: process-payment
    service: payment-server
    method: PaymentService.Charge
    compensate: PaymentService.Refund
    timeout: 30s
    retry:
      max_attempts: 3
      backoff: exponential

  - name: confirm-order
    service: order-server
    method: OrderService.Confirm
    compensate: OrderService.Cancel
    timeout: 30s
    retry:
      max_attempts: 3
      backoff: exponential
```

### イベントスキーマ設計（Protobuf）

#### スキーマ進化ルール

Protobuf のスキーマ進化は以下のルールに従う（Schema Registry の BACKWARD 互換性モードに準拠）。

| 変更種別           | 互換性 | 対応方法                     |
| ------------------ | ------ | ---------------------------- |
| フィールド追加     | 互換   | 新しいフィールド番号で追加   |
| フィールド削除     | 互換   | `reserved` で番号を予約      |
| フィールド型変更   | 非互換 | 新バージョンのトピックを作成 |
| フィールド名変更   | 互換   | ワイヤーフォーマットに影響なし |

### Go 実装パターン

#### Producer

```go
// internal/infra/messaging/producer.go
package messaging

import (
    "context"

    "github.com/segmentio/kafka-go"
    "google.golang.org/protobuf/proto"
)

type EventProducer struct {
    writer *kafka.Writer
}

func NewEventProducer(brokers []string, topic string) *EventProducer {
    return &EventProducer{
        writer: &kafka.Writer{
            Addr:         kafka.TCP(brokers...),
            Topic:        topic,
            Balancer:     &kafka.Hash{},   // パーティションキーによる分散
            RequiredAcks: kafka.RequireAll, // acks=all
            Async:        false,
        },
    }
}

func (p *EventProducer) Publish(ctx context.Context, key string, event proto.Message) error {
    value, err := proto.Marshal(event)
    if err != nil {
        return fmt.Errorf("marshal event: %w", err)
    }

    return p.writer.WriteMessages(ctx, kafka.Message{
        Key:   []byte(key),
        Value: value,
    })
}
```

#### Consumer

```go
// internal/infra/messaging/consumer.go
package messaging

import (
    "context"
    "log/slog"

    "github.com/segmentio/kafka-go"
)

type EventConsumer struct {
    reader *kafka.Reader
    logger *slog.Logger
}

func NewEventConsumer(brokers []string, topic, groupID string, logger *slog.Logger) *EventConsumer {
    return &EventConsumer{
        reader: kafka.NewReader(kafka.ReaderConfig{
            Brokers:  brokers,
            Topic:    topic,
            GroupID:  groupID,
            MinBytes: 1e3,    // 1KB
            MaxBytes: 10e6,   // 10MB
        }),
        logger: logger,
    }
}

func (c *EventConsumer) Consume(ctx context.Context, handler func(context.Context, kafka.Message) error) error {
    for {
        msg, err := c.reader.ReadMessage(ctx)
        if err != nil {
            return fmt.Errorf("read message: %w", err)
        }

        if err := handler(ctx, msg); err != nil {
            c.logger.Error("failed to handle message",
                slog.String("topic", msg.Topic),
                slog.Int("partition", msg.Partition),
                slog.Int64("offset", msg.Offset),
                slog.String("error", err.Error()),
            )
            // DLQ への転送はリトライ後に実施
        }
    }
}
```

### Rust 実装パターン

#### Producer

```rust
// src/infra/messaging/producer.rs
use prost::Message;
use rdkafka::producer::{FutureProducer, FutureRecord};
use rdkafka::ClientConfig;
use std::time::Duration;

pub struct EventProducer {
    producer: FutureProducer,
    topic: String,
}

impl EventProducer {
    pub fn new(brokers: &str, topic: &str) -> Self {
        let producer: FutureProducer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("acks", "all")
            .set("message.timeout.ms", "5000")
            .create()
            .expect("Failed to create producer");

        Self {
            producer,
            topic: topic.to_string(),
        }
    }

    pub async fn publish<M: Message>(&self, key: &str, event: &M) -> Result<(), Box<dyn std::error::Error>> {
        let payload = event.encode_to_vec();

        self.producer
            .send(
                FutureRecord::to(&self.topic)
                    .key(key)
                    .payload(&payload),
                Duration::from_secs(5),
            )
            .await
            .map_err(|(e, _)| e)?;

        Ok(())
    }
}
```

#### Consumer

```rust
// src/infra/messaging/consumer.rs
use rdkafka::consumer::{CommitMode, Consumer, StreamConsumer};
use rdkafka::{ClientConfig, Message};
use futures::StreamExt;

pub struct EventConsumer {
    consumer: StreamConsumer,
}

impl EventConsumer {
    pub fn new(brokers: &str, group_id: &str, topics: &[&str]) -> Self {
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("group.id", group_id)
            .set("auto.offset.reset", "earliest")
            .set("enable.auto.commit", "false")
            .create()
            .expect("Failed to create consumer");

        consumer.subscribe(topics).expect("Failed to subscribe");

        Self { consumer }
    }

    pub async fn consume<F, Fut>(&self, handler: F) -> Result<(), Box<dyn std::error::Error>>
    where
        F: Fn(Vec<u8>) -> Fut,
        Fut: std::future::Future<Output = Result<(), Box<dyn std::error::Error>>>,
    {
        let mut stream = self.consumer.stream();

        while let Some(result) = stream.next().await {
            let msg = result?;
            if let Some(payload) = msg.payload() {
                handler(payload.to_vec()).await?;
                self.consumer.commit_message(&msg, CommitMode::Async)?;
            }
        }

        Ok(())
    }
}
```

---

## 関連ドキュメント

- [tier-architecture.md](tier-architecture.md) — Tier アーキテクチャの詳細
- [API設計.md](API設計.md) — REST API・gRPC・GraphQL 設計
- [kubernetes設計.md](kubernetes設計.md) — Namespace 設計（messaging NS）
- [可観測性設計.md](可観測性設計.md) — Kafka メトリクス・ログ設計
- [サービスメッシュ設計.md](サービスメッシュ設計.md) — サービス間通信の耐障害性
- [CI-CD設計.md](CI-CD設計.md) — Proto スキーマの CI チェック
- [config設計.md](config設計.md) — config.yaml スキーマ（kafka セクション）
- [docker-compose設計.md](docker-compose設計.md) — ローカル開発環境の Kafka/Schema Registry 設定
